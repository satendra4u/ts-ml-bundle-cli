
bundle:
  name: {{ project_name }}

artifacts:
  default:
    path: .
    build: "pip install -r requirements.txt -t ./dist && echo built"

variables:
  project_name:
    default: {{ project_name_underscore }}
  serving_endpoint_name:
    default: ${var.project_name}-endpoint
  model_name:
    default: ${var.project_name}_model
  cluster_policy_restricted:
    default: POLICY_ID_PLACEHOLDER

# ----- Targets (envs) -----
targets:
  dev:
    workspace:
      host: {{ workspace_host }}
    default: true
    permissions:
      - level: CAN_VIEW
        group_name: ds-dev
      - level: CAN_MANAGE
        group_name: mlops-dev

  stg:
    workspace:
      host: {{ workspace_host }}
    permissions:
      - level: CAN_VIEW
        group_name: ds-stg
      - level: CAN_MANAGE
        group_name: mlops-stg

  prod:
    workspace:
      host: {{ workspace_host }}
    permissions:
      - level: CAN_VIEW
        group_name: readers
      - level: CAN_MANAGE
        group_name: mlops-prod

# ----- Shared Resources -----
resources:
  registered_models:
    {{ project_name_underscore }}_model:
      name: ${var.model_name}
      comment: "{{ model_type|title }} model - UC backed"

  jobs:
    {{ project_name_underscore }}_01_preprocess:
      name: ${var.project_name}-01-preprocess
      tasks:
        - task_key: preprocess
          python_wheel_task:
            package_name: ds
            entry_point: preprocess
          libraries:
            - whl: dist
          job_cluster_key: jc_{% if use_gpu %}gpu{% else %}standard{% endif %}_cluster
      job_clusters:
        - job_cluster_key: jc_{% if use_gpu %}gpu{% else %}standard{% endif %}_cluster
          new_cluster:
            spark_version: {{ spark_version }}
            node_type_id: {{ node_type }}
            num_workers: 0
            data_security_mode: SINGLE_USER
            runtime_engine: STANDARD
            {% if use_gpu %}
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: local[*]
            custom_tags:
              ResourceClass: SingleNode
            {% endif %}

    {{ project_name_underscore }}_02_train:
      name: ${var.project_name}-02-train
      tasks:
        - task_key: train
          python_wheel_task:
            package_name: ds
            entry_point: train
          libraries:
            - whl: dist
          job_cluster_key: jc_{% if use_gpu %}gpu{% else %}standard{% endif %}_cluster
      job_clusters:
        - job_cluster_key: jc_{% if use_gpu %}gpu{% else %}standard{% endif %}_cluster
          new_cluster:
            spark_version: {{ spark_version }}
            node_type_id: {{ node_type }}
            num_workers: 0
            data_security_mode: SINGLE_USER
            {% if use_gpu %}
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: local[*]
            custom_tags:
              ResourceClass: SingleNode
            {% endif %}

    {{ project_name_underscore }}_03_register:
      name: ${var.project_name}-03-register
      tasks:
        - task_key: register
          python_wheel_task:
            package_name: ds
            entry_point: register
          libraries:
            - whl: dist
          job_cluster_key: jc_standard_cluster
      job_clusters:
        - job_cluster_key: jc_standard_cluster
          new_cluster:
            spark_version: 14.3.x-scala2.12
            node_type_id: i3.xlarge
            num_workers: 0
            data_security_mode: SINGLE_USER
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: local[*]
            custom_tags:
              ResourceClass: SingleNode

    {{ project_name_underscore }}_04_deploy_serving:
      name: ${var.project_name}-04-deploy-serving
      tasks:
        - task_key: deploy_serving
          python_wheel_task:
            package_name: ds
            entry_point: deploy_serving
          libraries:
            - whl: dist
          job_cluster_key: jc_standard_cluster
      job_clusters:
        - job_cluster_key: jc_standard_cluster
          new_cluster:
            spark_version: 14.3.x-scala2.12
            node_type_id: i3.xlarge
            num_workers: 0
            data_security_mode: SINGLE_USER
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: local[*]
            custom_tags:
              ResourceClass: SingleNode

    {{ project_name_underscore }}_05_batch_inference:
      name: ${var.project_name}-05-batch-inference
      tasks:
        - task_key: batch_inference
          notebook_task:
            notebook_path: /notebooks/04_batch_inference.ipynb
          job_cluster_key: jc_{% if use_gpu %}gpu{% else %}standard{% endif %}_cluster
      job_clusters:
        - job_cluster_key: jc_{% if use_gpu %}gpu{% else %}standard{% endif %}_cluster
          new_cluster:
            spark_version: {{ spark_version }}
            node_type_id: {{ node_type }}
            num_workers: 0
            data_security_mode: SINGLE_USER
            {% if use_gpu %}
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: local[*]
            custom_tags:
              ResourceClass: SingleNode
            {% endif %}

  model_serving_endpoints:
    {{ project_name_underscore }}_endpoint:
      name: ${var.serving_endpoint_name}
      config:
        served_models:
          - name: ${var.model_name}
            model_name: ${var.model_name}
            model_version: "1"
            scale_to_zero_enabled: true
            workload_size: "Medium"
            {% if use_gpu %}workload_type: "GPU_MEDIUM"{% endif %}
